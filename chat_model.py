import time

from langchain.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.chains import RetrievalQA
from langchain.memory import ConversationBufferWindowMemory

from utils import llm_choice, extract_text
import streamlit as st
vector_store = None


def ingest(key: str, files_list=None, video_text=None) -> None:
    """
    Adds PDFs or videos to a vector store through embeddings.
    For files, it extracts text from each one and concatenates them.
    For videos, the text should be provided via the `video_text` parameter, which is obtained beforehand
    by calling the transcription function.

    The text is then divided into chunks. These chunks are further divided into batches, with each batch containing
    180 chunks. This is done to ensure that the API processes each batch with a time interval between them since there
    is a limit on the number of calls a user can make per minute.

    If there is only one batch with 180 chunks, it is used to ingest into the vector store. If the vector store has been
    populated before, a new vector store is created and merged with the existing one. If there are multiple batches,
    they are processed one by one with a 60-second interval between them. Each batch generates a separate vector store,
    and these vector stores are stored in a list.

    If the `vector_store` is `None`, it will take the first element from the list as the initial vector store and
    proceed to merge it with the others. If it is not `None`, it will use the existing vector store and merge it
    with all the elements from the list.

    :param key: OpenAI key used for the embeddings
    :param files_list: List of files that the user uploads
    :param video_text: Text of the transcription of the YouTube video
    :return: None
    """
    global vector_store
    if files_list is not None:
        text = ""
        for file in files_list:
            text_file = extract_text(file)
            text += text_file

    else:
        text = video_text

    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=2000,
        chunk_overlap=100,
        length_function=len
    )

    chunks = text_splitter.split_text(text)
    batches = [chunks[i:i + 180] for i in range(0, len(chunks), 180)]

    # HuggingFaceInstructEmbeddings(model_name="hkunlp/instructor-xl")
    embeddings = OpenAIEmbeddings(openai_api_key=key)

    if len(batches) == 1:
        if vector_store is None:
            vector_store = FAISS.from_texts(batches[0], embedding=embeddings)
        else:
            another_vector_store = FAISS.from_texts(batches[0], embedding=embeddings)
            vector_store.merge_from(another_vector_store)
    else:
        vectorstore_list = []
        for batch_index, batch in enumerate(batches):
            vector_store_batch = FAISS.from_texts(batch, embedding=embeddings)
            vectorstore_list.append(vector_store_batch)
            if batch_index < len(batches) - 1:
                time.sleep(61)

        if vector_store is None:
            vector_store = vectorstore_list[0]
            start_index = 1
        else:
            start_index = 0
        for vector in vectorstore_list[start_index:]:
            vector_store.merge_from(vector)


def get_response(llm_name: str, key: str, prompt: str) -> str:
    """
    Retrieves the response generated by the chosen Large Language Model (LLM) by specifying the LLM using 'llm_name.'
    This process involves creating a memory that stores only the preceding message in the conversation.
    The 'vector_store' serves as the retrieval mechanism, and the 'RetrievalQA' chain is utilized to extract
    the response from the selected LLM.

    :param llm_name: Name of the desired LLM.
    :param key: OpenAI API key.
    :param prompt: User's input prompt.
    :return: Response generated by the LLM.
    """
    try:
        llm = llm_choice(llm_name, key, "chat")
        memory = ConversationBufferWindowMemory(k=1)
        retriever = vector_store.as_retriever(search_kwargs={'k': 4})
        qa = RetrievalQA.from_chain_type(llm=llm, chain_type="stuff", retriever=retriever, memory=memory)
        response = qa.run(prompt)
        return response
    except Exception as e:
        st.error(str(e))
        st.stop()
